{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82575229",
   "metadata": {},
   "source": [
    "# CO542 - Neural Networks and Fuzzy Systems\n",
    "## E/19/129 - K.H. Gunawardana\n",
    "\n",
    "### Lab 02: Multi-Layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9e75d",
   "metadata": {},
   "source": [
    "### **Task 01: Implementing an MLP for the XOR Problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18e76f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8f0130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XOR problem's input and output data.\n",
    "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y = [0, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b524050",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(5,),    # One hidden layer with 5 neurons\n",
    "    activation='relu',          # ReLU activation for hidden layer\n",
    "    max_iter=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7197690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(5,), max_iter=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(5,), max_iter=500)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(5,), max_iter=500)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b90bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 5 hidden neurons: 1.0\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X)\n",
    "print(\"Accuracy with 5 hidden neurons:\", accuracy_score(y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9df99",
   "metadata": {},
   "source": [
    "### **Questions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a3908",
   "metadata": {},
   "source": [
    "#### What are the weights and biases after training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52480bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training:\n",
      "Layer 1 weights:\n",
      "[[-7.58027764e-01  3.51667402e-12  7.36264056e-02 -1.04431977e+00\n",
      "  -9.66284545e-02]\n",
      " [-5.69877737e-02 -9.68075974e-06 -2.27924334e-02 -1.04380488e+00\n",
      "   6.37640708e-01]]\n",
      "\n",
      "Layer 2 weights:\n",
      "[[ 0.57236868]\n",
      " [ 0.49275298]\n",
      " [-0.32651145]\n",
      " [-0.9209957 ]\n",
      " [-0.56935906]]\n",
      "\n",
      "Baises after training:\n",
      "Layer 1 biases:\n",
      "[ 0.82299905 -0.15263851 -0.41408271  1.0440735   0.82203451]\n",
      "\n",
      "Layer 2 biases:\n",
      "[0.60969224]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights after training:\")\n",
    "for i, w in enumerate(model.coefs_):\n",
    "    print(f\"Layer {i + 1} weights:\\n{w}\\n\")\n",
    "\n",
    "print(\"Baises after training:\")\n",
    "for i, b in enumerate(model.intercepts_):\n",
    "    print(f\"Layer {i + 1} biases:\\n{b}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14de8b6",
   "metadata": {},
   "source": [
    "#### Why is a hidden layer necessary for this problem?\n",
    "\n",
    "- A hidden layer is necessary for solving the XOR problem because the XOR pattern is not linearly separable. A simple neural network without a hidden layer can only solve problems where the data can be separated with a straight line. But in XOR, the inputs [0, 0] and [1, 1] belong to one class (output 0), and [0, 1] and [1, 0] belong to another (output 1), which makes the pattern more complex. \n",
    "- A hidden layer allows the model to create a more flexible boundary between classes by applying a non-linear function (like ReLU). This helps the network learn the correct relationship between the inputs and outputs. So, without a hidden layer, the model cannot solve the XOR problem correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e483aa6",
   "metadata": {},
   "source": [
    "#### What happens to the model performance if the hidden layer size is increased to 10 neurons?\n",
    "\n",
    "- When we increase the number of hidden neurons from 5 to 10 and retrain the model, the accuracy stays the same — 100% — because the XOR problem is small and simple. The model already learns the correct pattern with 5 neurons, so adding more neurons doesn’t improve accuracy. However, it does make the model slightly more complex, with more weights and biases to train. This can lead to longer training time, but the difference is usually small for such a tiny dataset. If we were working with a larger or noisier dataset, having too many neurons could cause the model to overfit, meaning it would perform well on the training data but poorly on new data. But in this case, since we're testing on the same data we trained on, the accuracy remains perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bab6f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_larger = MLPClassifier(hidden_layer_sizes=(10,), \n",
    "                             activation='relu', \n",
    "                             max_iter=500,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7adbddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 10 hidden neurons: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_larger.fit(X, y)\n",
    "preds = model_larger.predict(X)\n",
    "print(\"Accuracy with 10 hidden neurons:\", accuracy_score(y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2eb534",
   "metadata": {},
   "source": [
    "#### How does changing the activation function in the hidden layer to tanh affect the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d809088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with tanh: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_tanh = MLPClassifier(hidden_layer_sizes=(5,), \n",
    "                           activation='tanh', \n",
    "                           max_iter=500\n",
    "                        )\n",
    "model_tanh.fit(X, y)\n",
    "preds = model_tanh.predict(X)\n",
    "print(\"Accuracy with tanh:\", accuracy_score(y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb9714a",
   "metadata": {},
   "source": [
    "- When we change the activation function in the hidden layer from ReLU to tanh, the model still tries to learn the XOR pattern, but the learning behavior changes. Unlike ReLU, which only outputs positive values, tanh gives outputs between -1 and 1 and is symmetric around zero. This can help the model learn balanced patterns, but it can also cause the optimization process to get stuck if the gradients become too small (a problem known as vanishing gradients). So while tanh can work well in theory, in practice it may take longer to converge or might need some tuning, especially for small models or datasets like XOR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8355df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13438649",
   "metadata": {},
   "source": [
    "#### **Task 02: Predicting California Housing Prices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2896f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ffe3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Step 2: Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4445b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split data (90% training, 10% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "195eda12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define and train the MLPRegressor\n",
    "model = MLPRegressor(hidden_layer_sizes=(128, 64, 32),\n",
    "                     activation='relu',\n",
    "                     solver='adam',\n",
    "                     max_iter=300,\n",
    "                     random_state=42,\n",
    "                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04f18bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53885876\n",
      "Iteration 2, loss = 0.22183631\n",
      "Iteration 3, loss = 0.19042119\n",
      "Iteration 4, loss = 0.17935049\n",
      "Iteration 5, loss = 0.17190260\n",
      "Iteration 6, loss = 0.16665396\n",
      "Iteration 7, loss = 0.16174409\n",
      "Iteration 8, loss = 0.16063601\n",
      "Iteration 9, loss = 0.15373855\n",
      "Iteration 10, loss = 0.14980380\n",
      "Iteration 11, loss = 0.14803004\n",
      "Iteration 12, loss = 0.14469742\n",
      "Iteration 13, loss = 0.14767503\n",
      "Iteration 14, loss = 0.14122103\n",
      "Iteration 15, loss = 0.13989063\n",
      "Iteration 16, loss = 0.13917637\n",
      "Iteration 17, loss = 0.13873904\n",
      "Iteration 18, loss = 0.13644931\n",
      "Iteration 19, loss = 0.13520772\n",
      "Iteration 20, loss = 0.13558742\n",
      "Iteration 21, loss = 0.13216058\n",
      "Iteration 22, loss = 0.13281505\n",
      "Iteration 23, loss = 0.13157710\n",
      "Iteration 24, loss = 0.13182753\n",
      "Iteration 25, loss = 0.12978768\n",
      "Iteration 26, loss = 0.12913776\n",
      "Iteration 27, loss = 0.13141475\n",
      "Iteration 28, loss = 0.12975822\n",
      "Iteration 29, loss = 0.13233942\n",
      "Iteration 30, loss = 0.12626563\n",
      "Iteration 31, loss = 0.12690679\n",
      "Iteration 32, loss = 0.12593240\n",
      "Iteration 33, loss = 0.12447905\n",
      "Iteration 34, loss = 0.12296302\n",
      "Iteration 35, loss = 0.12356680\n",
      "Iteration 36, loss = 0.12441836\n",
      "Iteration 37, loss = 0.12167488\n",
      "Iteration 38, loss = 0.12280931\n",
      "Iteration 39, loss = 0.12056385\n",
      "Iteration 40, loss = 0.12016694\n",
      "Iteration 41, loss = 0.12015531\n",
      "Iteration 42, loss = 0.11962212\n",
      "Iteration 43, loss = 0.12112122\n",
      "Iteration 44, loss = 0.11955391\n",
      "Iteration 45, loss = 0.11891423\n",
      "Iteration 46, loss = 0.12069857\n",
      "Iteration 47, loss = 0.11870553\n",
      "Iteration 48, loss = 0.11768062\n",
      "Iteration 49, loss = 0.11851117\n",
      "Iteration 50, loss = 0.11731875\n",
      "Iteration 51, loss = 0.11757828\n",
      "Iteration 52, loss = 0.11553725\n",
      "Iteration 53, loss = 0.11613891\n",
      "Iteration 54, loss = 0.11602197\n",
      "Iteration 55, loss = 0.11507391\n",
      "Iteration 56, loss = 0.11438210\n",
      "Iteration 57, loss = 0.11591446\n",
      "Iteration 58, loss = 0.11332506\n",
      "Iteration 59, loss = 0.11286105\n",
      "Iteration 60, loss = 0.11414991\n",
      "Iteration 61, loss = 0.11379563\n",
      "Iteration 62, loss = 0.11149269\n",
      "Iteration 63, loss = 0.11292647\n",
      "Iteration 64, loss = 0.11304088\n",
      "Iteration 65, loss = 0.11060639\n",
      "Iteration 66, loss = 0.11113753\n",
      "Iteration 67, loss = 0.11257060\n",
      "Iteration 68, loss = 0.11119158\n",
      "Iteration 69, loss = 0.11022458\n",
      "Iteration 70, loss = 0.11028686\n",
      "Iteration 71, loss = 0.10820808\n",
      "Iteration 72, loss = 0.10815655\n",
      "Iteration 73, loss = 0.10969768\n",
      "Iteration 74, loss = 0.10829090\n",
      "Iteration 75, loss = 0.10740772\n",
      "Iteration 76, loss = 0.10770249\n",
      "Iteration 77, loss = 0.10671158\n",
      "Iteration 78, loss = 0.10892393\n",
      "Iteration 79, loss = 0.10829539\n",
      "Iteration 80, loss = 0.10667652\n",
      "Iteration 81, loss = 0.10615972\n",
      "Iteration 82, loss = 0.10531079\n",
      "Iteration 83, loss = 0.10847395\n",
      "Iteration 84, loss = 0.10489063\n",
      "Iteration 85, loss = 0.10589620\n",
      "Iteration 86, loss = 0.10354055\n",
      "Iteration 87, loss = 0.10461213\n",
      "Iteration 88, loss = 0.10270173\n",
      "Iteration 89, loss = 0.10211239\n",
      "Iteration 90, loss = 0.10115603\n",
      "Iteration 91, loss = 0.10172715\n",
      "Iteration 92, loss = 0.10313921\n",
      "Iteration 93, loss = 0.10174300\n",
      "Iteration 94, loss = 0.10265343\n",
      "Iteration 95, loss = 0.10047251\n",
      "Iteration 96, loss = 0.10079583\n",
      "Iteration 97, loss = 0.10023603\n",
      "Iteration 98, loss = 0.10000556\n",
      "Iteration 99, loss = 0.10039013\n",
      "Iteration 100, loss = 0.10061450\n",
      "Iteration 101, loss = 0.09870890\n",
      "Iteration 102, loss = 0.10019482\n",
      "Iteration 103, loss = 0.09995712\n",
      "Iteration 104, loss = 0.09857626\n",
      "Iteration 105, loss = 0.10068220\n",
      "Iteration 106, loss = 0.09774448\n",
      "Iteration 107, loss = 0.09975038\n",
      "Iteration 108, loss = 0.09709050\n",
      "Iteration 109, loss = 0.09763434\n",
      "Iteration 110, loss = 0.09611564\n",
      "Iteration 111, loss = 0.09585021\n",
      "Iteration 112, loss = 0.09578515\n",
      "Iteration 113, loss = 0.09773478\n",
      "Iteration 114, loss = 0.09631580\n",
      "Iteration 115, loss = 0.09464364\n",
      "Iteration 116, loss = 0.09549307\n",
      "Iteration 117, loss = 0.09560952\n",
      "Iteration 118, loss = 0.09488463\n",
      "Iteration 119, loss = 0.09648996\n",
      "Iteration 120, loss = 0.09426029\n",
      "Iteration 121, loss = 0.09376961\n",
      "Iteration 122, loss = 0.09412505\n",
      "Iteration 123, loss = 0.09315444\n",
      "Iteration 124, loss = 0.09270558\n",
      "Iteration 125, loss = 0.09390696\n",
      "Iteration 126, loss = 0.09350071\n",
      "Iteration 127, loss = 0.09247622\n",
      "Iteration 128, loss = 0.09141526\n",
      "Iteration 129, loss = 0.09198316\n",
      "Iteration 130, loss = 0.09362642\n",
      "Iteration 131, loss = 0.09249794\n",
      "Iteration 132, loss = 0.09140421\n",
      "Iteration 133, loss = 0.09127482\n",
      "Iteration 134, loss = 0.09039659\n",
      "Iteration 135, loss = 0.09111455\n",
      "Iteration 136, loss = 0.09071146\n",
      "Iteration 137, loss = 0.09016396\n",
      "Iteration 138, loss = 0.08868109\n",
      "Iteration 139, loss = 0.09047102\n",
      "Iteration 140, loss = 0.09009896\n",
      "Iteration 141, loss = 0.09050268\n",
      "Iteration 142, loss = 0.08973752\n",
      "Iteration 143, loss = 0.08886754\n",
      "Iteration 144, loss = 0.08870112\n",
      "Iteration 145, loss = 0.08952560\n",
      "Iteration 146, loss = 0.08937618\n",
      "Iteration 147, loss = 0.09937368\n",
      "Iteration 148, loss = 0.08872855\n",
      "Iteration 149, loss = 0.09040636\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=(128, 64, 32), max_iter=300, random_state=42,\n",
       "             verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(hidden_layer_sizes=(128, 64, 32), max_iter=300, random_state=42,\n",
       "             verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(128, 64, 32), max_iter=300, random_state=42,\n",
       "             verbose=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea01fd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Set: 0.27039027460951764\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 5: Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "\n",
    "# Step 6: Visualize loss curve\n",
    "plt.plot(model.loss_curve_)\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdeecc6",
   "metadata": {},
   "source": [
    "### **Questions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a0222b",
   "metadata": {},
   "source": [
    "#### What is the mean squared error on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d16dfe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Set: 0.27039027460951764\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Squared Error on Test Set:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7564f",
   "metadata": {},
   "source": [
    "- The Mean Squared Error (MSE) measures the average squared difference between actual and predicted values in regression models, with lower values indicating better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717733b",
   "metadata": {},
   "source": [
    "#### How does normalization affect model performance?\n",
    "\n",
    "Normalization is a crucial preprocessing step in training neural networks, significantly impacting model performance. Here's how it helps:\n",
    "- Equalizes Feature Scales: In datasets like California housing, features such as income and house age can have vastly different scales. Normalization adjusts these features to a common scale, ensuring that no single feature disproportionately influences the model's learning process.\n",
    "- Accelerates Convergence: By scaling features, normalization allows the optimization algorithm to converge more quickly. It prevents issues like vanishing or exploding gradients, which can hinder the training process. This leads to faster and more stable training.\n",
    "- Enhances Generalization: Normalized data helps the model generalize better to unseen data by preventing overfitting. It ensures that the model captures the underlying patterns rather than memorizing the training data.\n",
    "- Improves Numerical Stability: Normalization keeps input values within a range that maintains numerical stability during computations, reducing the risk of computational errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d7ccb0",
   "metadata": {},
   "source": [
    "#### How does changing the solver from Adam to SGD affect training time and accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac730957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Optimizer:\n",
      "Training Time: 73.09 seconds\n",
      "Test MSE: 0.2704\n",
      "\n",
      "SGD Optimizer:\n",
      "Training Time: 151.35 seconds\n",
      "Test MSE: 0.2823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "hidden_layers = (128, 64, 32)\n",
    "max_iter = 300\n",
    "random_state = 42\n",
    "\n",
    "# Train with Adam optimizer\n",
    "start_time = time.time()\n",
    "model_adam = MLPRegressor(\n",
    "    hidden_layer_sizes=hidden_layers,\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=max_iter,\n",
    "    random_state=random_state\n",
    ")\n",
    "model_adam.fit(X_train, y_train)\n",
    "adam_time = time.time() - start_time\n",
    "adam_mse = mean_squared_error(y_test, model_adam.predict(X_test))\n",
    "\n",
    "print(f\"Adam Optimizer:\\nTraining Time: {adam_time:.2f} seconds\\nTest MSE: {adam_mse:.4f}\\n\")\n",
    "\n",
    "# Train with SGD optimizer\n",
    "start_time = time.time()\n",
    "model_sgd = MLPRegressor(\n",
    "    hidden_layer_sizes=hidden_layers,\n",
    "    activation='relu',\n",
    "    solver='sgd',\n",
    "    learning_rate_init=0.01,\n",
    "    max_iter=max_iter,\n",
    "    random_state=random_state\n",
    ")\n",
    "model_sgd.fit(X_train, y_train)\n",
    "sgd_time = time.time() - start_time\n",
    "sgd_mse = mean_squared_error(y_test, model_sgd.predict(X_test))\n",
    "\n",
    "print(f\"SGD Optimizer:\\nTraining Time: {sgd_time:.2f} seconds\\nTest MSE: {sgd_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd5bf9",
   "metadata": {},
   "source": [
    "Adam Optimizer\n",
    "- The Adam optimizer completed training in approximately 73.09 seconds, achieving a Mean Squared Error (MSE) of 0.2704 on the test set. Adam's adaptive learning rate mechanism allows it to adjust the learning rate for each parameter dynamically, leading to faster convergence and often better performance on complex datasets. This efficiency makes Adam a popular choice for training deep learning models, especially when quick convergence is desired.\n",
    "MachineLearningMastery.com\n",
    "\n",
    "SGD Optimizer\n",
    "- In contrast, the Stochastic Gradient Descent (SGD) optimizer took about 151.35 seconds to train the model, resulting in a slightly higher MSE of 0.2823 on the test set. SGD updates parameters using the gradient of the loss function, which can lead to slower convergence, especially without fine-tuned hyperparameters like learning rate and momentum. While SGD can generalize well in certain scenarios, it often requires more careful tuning and longer training times.\n",
    "\n",
    "Conclusion\n",
    "- For the California housing price prediction task, the Adam optimizer outperformed SGD in both training time and model accuracy. Adam's ability to adaptively adjust learning rates contributed to faster convergence and better performance, making it a suitable choice for this regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d5efa",
   "metadata": {},
   "source": [
    "#### How does the model performance change if the dataset is split as 70% training and 30% testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1307d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 138.05 seconds\n",
      "Test Mean Squared Error: 0.2780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into 70% training and 30% testing\n",
    "X_train_70, X_test_30, y_train_70, y_test_30 = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the MLPRegressor model\n",
    "mlp_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model and record the training time\n",
    "start_time = time.time()\n",
    "mlp_model.fit(X_train_70, y_train_70)\n",
    "training_duration = time.time() - start_time\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = mlp_model.predict(X_test_30)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test_30, predictions)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Training Time: {training_duration:.2f} seconds\")\n",
    "print(f\"Test Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fec6c7",
   "metadata": {},
   "source": [
    "- Adjusting the dataset split to 70% training and 30% testing resulted in a training time of approximately 138.05 seconds and a Mean Squared Error (MSE) of 0.2780 on the test set. This indicates that the model maintained strong predictive performance despite having less training data. The larger test set provides a more comprehensive evaluation, ensuring the model's generalization capability is accurately assessed.\n",
    "\n",
    "Overall, the model demonstrates robust performance with the 70-30 split, balancing training efficiency and predictive accuracy effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4bf319",
   "metadata": {},
   "source": [
    "#### What is the effect of increasing the number of hidden layers to (256, 128, 64, 32)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41b12eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 298.28 seconds\n",
      "Test Mean Squared Error: 0.2922\n"
     ]
    }
   ],
   "source": [
    "model_ = MLPRegressor(hidden_layer_sizes=(256, 128, 64, 32),\n",
    "                     activation='relu',\n",
    "                     solver='adam',\n",
    "                     max_iter=300,\n",
    "                     random_state=42)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "model_.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model_.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"Test Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa809ca",
   "metadata": {},
   "source": [
    "Increasing the number of hidden layers in the neural network to a configuration of (256, 128, 64, 32) resulted in a training time of approximately 298.28 seconds and a test Mean Squared Error (MSE) of 0.2922. This deeper architecture enhances the model's capacity to learn complex patterns within the data. However, it's important to note that:\n",
    "\n",
    "- Training Time: The extended training duration is a direct consequence of the increased number of parameters and computations required in a deeper network.\n",
    "\n",
    "- Model Performance: While the deeper model has a higher capacity to fit the training data, it doesn't necessarily translate to improved performance on unseen data. The slight increase in MSE suggests that the added complexity may not provide significant benefits for this particular task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ba77f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13b3ee85",
   "metadata": {},
   "source": [
    "### **Task 03: Image Classification with MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cf96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0c3434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist.data, mnist.target.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9aff7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Filter the dataset to include only digits 0 and 1\n",
    "mask = (y == 0) | (y == 1)\n",
    "X_binary = X[mask]\n",
    "y_binary = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a90f5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Split the data into 80% training and 20% testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_binary, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ad401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create and train the MLPClassifier\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    activation='relu',\n",
    "    solver='sgd',\n",
    "    learning_rate_init=0.01,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a78513b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(256, 128, 64), learning_rate_init=0.01,\n",
       "              max_iter=300, random_state=42, solver=&#x27;sgd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(256, 128, 64), learning_rate_init=0.01,\n",
       "              max_iter=300, random_state=42, solver=&#x27;sgd&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(256, 128, 64), learning_rate_init=0.01,\n",
       "              max_iter=300, random_state=42, solver='sgd')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "426de5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[   0 1381]\n",
      " [   0 1575]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1381\n",
      "           1       0.53      1.00      0.70      1575\n",
      "\n",
      "    accuracy                           0.53      2956\n",
      "   macro avg       0.27      0.50      0.35      2956\n",
      "weighted avg       0.28      0.53      0.37      2956\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluate the model\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4bd3d",
   "metadata": {},
   "source": [
    "### **Questions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e447fb94",
   "metadata": {},
   "source": [
    "#### What is the accuracy of the test set?\n",
    "\n",
    "- The test set accuracy achieved by the MLPClassifier in your binary classification task (digits 0 vs. 1) is approximately 53%. This performance is notably low, especially considering that binary classification tasks on the MNIST dataset typically achieve accuracies exceeding 95% with appropriate configurations.\n",
    "\n",
    "- The confusion matrix indicates that the model predicted all test samples as class '1', failing to identify any instances of class '0'. This suggests a significant issue with the model's learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a32a7",
   "metadata": {},
   "source": [
    "#### How does the choice of activation function affect performance?\n",
    "\n",
    "- The choice of activation function impacts neural network performance, affecting learning, training efficiency, and accuracy. Using ReLU, your model predicted all test samples as class '1', yielding ~53% accuracy, possibly due to \"dying ReLU\" from improper initialization, high learning rates, or unnormalized data. Alternatives like tanh or sigmoid may help by providing smoother gradients, though they risk vanishing gradients in deep networks. Experimentation is crucial to select the best activation function for your task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef65de4",
   "metadata": {},
   "source": [
    "#### What happens to the classification performance if dropout is added to the model?\n",
    "\n",
    "- Adding dropout to a neural network can enhance its generalization by preventing overfitting, as it randomly deactivates neurons during training. This encourages the model to learn more robust features, potentially improving performance on unseen data. However, dropout may slow down training convergence and requires careful tuning; excessive dropout can lead to underfitting, while insufficient dropout might not effectively mitigate overfitting. Therefore, selecting an appropriate dropout rate is crucial to balance training efficiency and model generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df868fd",
   "metadata": {},
   "source": [
    "#### How does increasing the training data size to 90% impact the test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d12a0294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 13.01 seconds\n",
      "Test Accuracy: 0.5332\n",
      "Confusion Matrix:\n",
      "[[  0 690]\n",
      " [  0 788]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       690\n",
      "           1       0.53      1.00      0.70       788\n",
      "\n",
      "    accuracy                           0.53      1478\n",
      "   macro avg       0.27      0.50      0.35      1478\n",
      "weighted avg       0.28      0.53      0.37      1478\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Split the data: 90% training, 10% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_binary, y_binary, test_size=0.1, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "# Initialize the MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(256, 128, 64),\n",
    "                    activation='relu',\n",
    "                    solver='sgd',\n",
    "                    learning_rate_init=0.01,\n",
    "                    max_iter=100,\n",
    "                    random_state=42)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "mlp.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34646fc",
   "metadata": {},
   "source": [
    "- Increasing the training data size to 90% did not lead to an improvement in test accuracy in this particular case. The model achieved an accuracy of approximately 53.32%, which is similar to the results observed with a smaller training set. The confusion matrix reveals that the model failed to correctly classify any of the digit '0' instances and predicted all test samples as digit '1'. This is further confirmed by the classification report, which shows a precision, recall, and F1-score of 0.00 for class '0'.\n",
    "\n",
    "- This outcome suggests that the model has developed a strong bias toward one class, likely due to issues such as class imbalance, insufficient model complexity, or inappropriate learning dynamics (e.g., learning rate, solver). While increasing training data typically helps models generalize better, in this case, the underlying learning issues prevent the model from benefiting from the additional data. Therefore, simply increasing training size without addressing these limitations did not improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75f08e8",
   "metadata": {},
   "source": [
    "#### What do the misclassified samples indicate about the model's weaknesses?\n",
    "\n",
    "- The fact that every “0” was misclassified as “1” shows the model is heavily biased toward the majority class and isn’t learning the features that distinguish zeros from ones. This likely stems from class imbalance, inadequate feature discrimination, and possibly overfitting without regularization. To fix this, you’d balance the classes, improve feature learning (or model complexity), and add techniques like dropout to help the network generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf11d325",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba002ac9",
   "metadata": {},
   "source": [
    "### **Task 04: Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "093161d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566aa686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55da2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c370e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPRegressor(max_iter=200, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b3aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes': [(128, 64), (256, 128, 64)],\n",
    "    'mlp__activation': ['relu', 'tanh'],\n",
    "    'mlp__solver': ['adam', 'sgd']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af611128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Run GridSearchCV\n",
    "start_time = time.time()\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cbcfb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'mlp__activation': 'tanh', 'mlp__hidden_layer_sizes': (128, 64), 'mlp__solver': 'adam'}\n",
      "Best MSE Score: 0.2662369793698506\n",
      "Time Taken: 1013.54 seconds\n",
      "                                                                                          params  \\\n",
      "4       {'mlp__activation': 'tanh', 'mlp__hidden_layer_sizes': (128, 64), 'mlp__solver': 'adam'}   \n",
      "0       {'mlp__activation': 'relu', 'mlp__hidden_layer_sizes': (128, 64), 'mlp__solver': 'adam'}   \n",
      "6  {'mlp__activation': 'tanh', 'mlp__hidden_layer_sizes': (256, 128, 64), 'mlp__solver': 'adam'}   \n",
      "3   {'mlp__activation': 'relu', 'mlp__hidden_layer_sizes': (256, 128, 64), 'mlp__solver': 'sgd'}   \n",
      "2  {'mlp__activation': 'relu', 'mlp__hidden_layer_sizes': (256, 128, 64), 'mlp__solver': 'adam'}   \n",
      "1        {'mlp__activation': 'relu', 'mlp__hidden_layer_sizes': (128, 64), 'mlp__solver': 'sgd'}   \n",
      "7   {'mlp__activation': 'tanh', 'mlp__hidden_layer_sizes': (256, 128, 64), 'mlp__solver': 'sgd'}   \n",
      "5        {'mlp__activation': 'tanh', 'mlp__hidden_layer_sizes': (128, 64), 'mlp__solver': 'sgd'}   \n",
      "\n",
      "   mean_test_score  \n",
      "4        -0.266237  \n",
      "0        -0.272672  \n",
      "6        -0.278401  \n",
      "3        -0.290519  \n",
      "2        -0.293487  \n",
      "1        -0.315698  \n",
      "7        -0.326259  \n",
      "5        -0.375476  \n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best MSE Score:\", -grid_search.best_score_)\n",
    "print(\"Time Taken: {:.2f} seconds\".format(end_time - start_time))\n",
    "\n",
    "# Show all results\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "print(results_df[['params', 'mean_test_score']].sort_values(by='mean_test_score', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c40b9916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X_train, y_train, scoring='neg_mean_squared_error', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3da3c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation MSE Scores: [0.30761485 0.27595472 0.32713103 0.28231617 0.3280743 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross-Validation MSE Scores:\", -scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5500dc3",
   "metadata": {},
   "source": [
    "### **Questions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fcf12c",
   "metadata": {},
   "source": [
    "#### Which configuration gives the best performance?\n",
    "\n",
    "- The configuration that gives the best performance is the one with activation function tanh, hidden layer sizes (128, 64), and solver adam. This combination achieved the lowest mean squared error (MSE) score of 0.2662, indicating the most accurate predictions among all tested configurations. It outperformed other setups, including those with deeper networks or different activation functions and solvers, making it the optimal choice based on cross-validation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ff344",
   "metadata": {},
   "source": [
    "#### How does solver choice impact training speed and accuracy?\n",
    "\n",
    "Adam Solver:\n",
    "- The adam optimizer, which combines the advantages of AdaGrad and RMSProp, adapts learning rates for each parameter. This adaptability often leads to faster convergence and better performance on large datasets. In your tuning results, the configuration with adam, tanh activation, and hidden layers of (128, 64) achieved the best performance, with a mean squared error (MSE) of 0.2662. This suggests that adam is effective for the California Housing dataset, providing a good balance between training speed and accuracy.\n",
    "\n",
    "SGD Solver:\n",
    "- Stochastic Gradient Descent (sgd) updates parameters using the gradient of the loss function with respect to each parameter. While sgd can be faster per iteration due to its simplicity, it often requires careful tuning of the learning rate and may converge more slowly. In your experiments, configurations using sgd generally resulted in higher MSE scores compared to those using adam, indicating lower predictive accuracy.\n",
    "\n",
    "Conclusion:\n",
    "- For the California Housing Prices dataset, adam outperforms sgd in terms of both training speed and accuracy. Its adaptive learning rates and efficient convergence make it a suitable choice for this regression task. However, it's important to note that the optimal solver can vary depending on the specific characteristics of the dataset and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2cff93",
   "metadata": {},
   "source": [
    "#### What happens to the best configuration if the activation function is set to logistic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53b09bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE with logistic activation: 0.3294191931562107\n",
      "Cross-validation scores: [0.34079192 0.3190816  0.31502797 0.3105061  0.36168838]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the pipeline with StandardScaler and MLPRegressor using 'logistic' activation\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPRegressor(hidden_layer_sizes=(128, 64),\n",
    "                         activation='logistic',\n",
    "                         solver='adam',\n",
    "                         max_iter=200,\n",
    "                         random_state=42))\n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean MSE with logistic activation:\", -np.mean(scores))\n",
    "print(\"Cross-validation scores:\", -scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9616d93",
   "metadata": {},
   "source": [
    "- Switching the activation function to 'logistic' in the best-performing MLPRegressor configuration (which originally used 'tanh') led to a noticeable decline in performance and training efficiency. The mean cross-validation Mean Squared Error (MSE) increased from approximately 0.266 to 0.329, indicating reduced predictive accuracy. Additionally, convergence warnings were triggered, suggesting that the model did not fully converge within the set 200 iterations.\n",
    "\n",
    "- This outcome aligns with the known characteristics of the logistic (sigmoid) activation function. Unlike 'tanh', which outputs values between -1 and 1 and is zero-centered, 'logistic' outputs between 0 and 1 and is not zero-centered. This can lead to issues such as vanishing gradients, especially in deeper networks, resulting in slower convergence and potentially suboptimal model performance. Therefore, in this context, retaining the 'tanh' activation function is advisable for better training efficiency and predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5aad7e",
   "metadata": {},
   "source": [
    "#### How does increasing the maximum number of iterations to 500 affect performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fdbc8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE with logistic activation and max_iter=500: 0.27635510365706795\n",
      "Cross-validation scores: [0.27835606 0.27140814 0.27611925 0.26446604 0.29142603]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the pipeline with StandardScaler and MLPRegressor using increased max_iter\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPRegressor(hidden_layer_sizes=(128, 64),\n",
    "                         activation='logistic',\n",
    "                         solver='adam',\n",
    "                         max_iter=500,\n",
    "                         random_state=42))\n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Output the results\n",
    "print(\"Mean MSE with logistic activation and max_iter=500:\", -np.mean(scores))\n",
    "print(\"Cross-validation scores:\", -scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bf199a",
   "metadata": {},
   "source": [
    "- Increasing the max_iter parameter from 200 to 500 in the MLPRegressor with the 'logistic' activation function led to a modest improvement in performance. The mean cross-validation Mean Squared Error (MSE) decreased from approximately 0.329 to 0.276, indicating better predictive accuracy. However, convergence warnings persisted, suggesting that even with more iterations, the optimizer hadn't fully converged.\n",
    "\n",
    "- This outcome aligns with the characteristics of the 'logistic' activation function, which can lead to slower convergence due to issues like vanishing gradients. While increasing max_iter provided the model with more opportunities to minimize the loss function, it also resulted in longer training times. Therefore, while performance improved, the trade-off between training time and convergence should be considered.\n",
    "\n",
    "- In practice, using activation functions like 'tanh' or 'relu' may offer better convergence properties and faster training times. Additionally, implementing techniques such as early stopping or adjusting the learning rate could further enhance model performance and training efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351edda1",
   "metadata": {},
   "source": [
    "#### What is the impact of using a 3-fold cross-validation instead of a 5-fold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f060bdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-Fold CV Mean MSE: 0.4360935406325452\n",
      "3-Fold CV Scores: [0.39989306 0.35124376 0.5571438 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold CV Mean MSE: 0.3888482208310374\n",
      "5-Fold CV Scores: [0.34684872 0.4093356  0.36669381 0.36951854 0.45184443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define the pipeline with StandardScaler and MLPRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPRegressor(hidden_layer_sizes=(128, 64),\n",
    "                         activation='tanh',\n",
    "                         solver='adam',\n",
    "                         max_iter=200,\n",
    "                         random_state=42))\n",
    "])\n",
    "\n",
    "# Perform 3-fold cross-validation\n",
    "scores_3fold = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=3)\n",
    "mean_mse_3fold = -np.mean(scores_3fold)\n",
    "print(\"3-Fold CV Mean MSE:\", mean_mse_3fold)\n",
    "print(\"3-Fold CV Scores:\", -scores_3fold)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores_5fold = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=5)\n",
    "mean_mse_5fold = -np.mean(scores_5fold)\n",
    "print(\"5-Fold CV Mean MSE:\", mean_mse_5fold)\n",
    "print(\"5-Fold CV Scores:\", -scores_5fold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f7cd85",
   "metadata": {},
   "source": [
    "- Using 3-fold cross-validation instead of 5-fold in your MLPRegressor evaluation on the California Housing Prices dataset resulted in a higher mean MSE (0.4361 vs. 0.3888) and greater variability across folds. This suggests that 3-fold CV may provide less stable and potentially less reliable performance estimates compared to 5-fold CV.\n",
    "\n",
    "- The choice between 3-fold and 5-fold cross-validation involves a trade-off between computational efficiency and the reliability of performance estimates. While 3-fold CV requires fewer computations, it may lead to higher variance in the performance metrics, making the estimates more sensitive to specific data splits. On the other hand, 5-fold CV offers a better balance between bias and variance, providing more stable and reliable estimates of model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
